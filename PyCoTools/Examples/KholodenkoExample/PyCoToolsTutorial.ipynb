{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PyCoTools Tutorial** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial provides information on how to simulate a COPASI model, set up, run and analyse parameter estimations and calculate profile likelihoods using COPASI. Generally the architechture follows COPASI as much as possible with nomenclature and any optional arguments provided by COPASI are accessible via PyCoTools via use of keyword arguments. Each keyword argument has a default value that can be over-ridden by manually passing the keyword argument to a class. The Kholodenko2000 model is used as a show case model in this tutorial. PyCoTools interfaces with COPASI by modifying components of XML in the same way that COPASI does. This means that a model can simply be opened at any point during an analysis to varify things are working correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Installation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a command line or terminal with administrative privalleges, use:\n",
    "        > pip install PyCoTools\n",
    "        \n",
    "The source code is also available on [GitHub](https://github.com/CiaranWelsh/PyCoTools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input to most of the classes in the `pycopi` module are strings containing the path to a copasi file and a user  often requires custom output filenames (rather than the defaults), its common to have a numerous file paths to manage at any one time. One way to deal with this is:\n",
    "            1. Conduct your analysis from a new folder containing the .cps file of interest. \n",
    "            2. Create a blank `__init__.py' file in this folder\n",
    "            3. Create a module called (for instance) `FilePaths.py` \n",
    "            4) Place a class inside `FilePaths` containing paths relevant to your analysis. \n",
    "\n",
    "Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FilePaths.py\n",
    "class KholodenkoExample():\n",
    "    def __init__(self):\n",
    "        ## path to folder containing kholodenko example\n",
    "        if sys.platform=='win32':\n",
    "            self.current_directory='D:\\MPhil\\Python\\My_Python_Modules\\Modelling_Tools\\PyCoTools\\PyCoTools\\Examples\\KholodenkoExample'\n",
    "        else:\n",
    "            self.current_directory=r'/sharedlustre/users/b3053674/2017/Jan/Kholod/KholodenkoExample'\n",
    "        ## Kholodenko filename\n",
    "        self.copasi_filename=r'Kholodenko.cps'\n",
    "        ## full path to kholodenko model\n",
    "        self.kholodenko_model=os.path.join(self.current_directory,self.copasi_filename)\n",
    "        ## full path to the time course output\n",
    "        self.timecourse_report=os.path.join(self.current_directory,'KholodenkoTimeCourseOutput.txt')\n",
    "        ## full path to the noisy time course output\n",
    "        self.noisy_timecourse_report=os.path.join(self.current_directory,'NoisyKholodenkoTimeCourseOutput.txt')\n",
    "        ## Full path to parameter estimation results file\n",
    "        self.PEData_file=os.path.join(self.current_directory,'PEResultsFile.txt')\n",
    "        ## Full path to a folder containing all data from initial multiple global parameter estimations\n",
    "        self.PEData_dir=os.path.join(self.current_directory,'PEResults')\n",
    "        ## Full path to file containing secondary local parameter estimation (starting with best values from self.PEData_dir)\n",
    "        self.local_PEData_dir=os.path.join(self.current_directory,'LocalPEDataResults')\n",
    "        ## Pre-run parameter estimation data pickle file\n",
    "        self.PEData_pickle=os.path.join(self.current_directory,'KholodenkoExamplePEData.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These paths are now available  for the rest of the `.py` files in the current directory by instantiating an instance of `KholodenkoExample()` and 'dotting' into them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Time Course "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pycopi.TimeCourse` class enables a user to run an COPASI model deterministically. Future versions of PyCoTools will implement the alternative sovlers that COPASI offers which will be accessible via a keyword argument, but presently this feature is not implemented. \n",
    "\n",
    "From the directory containing FilePaths.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyCoTools\n",
    "from FilePaths import KholodenkoExample\n",
    "## allow plotting in this document\n",
    "%matplotlib inline\n",
    "\n",
    "## instantiate instance of KholodenoExample to manage directories \n",
    "K=KholodenkoExample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run deterministic time course with kholodenko model we have a few options. We can:\n",
    "    1. Collect all copasi output as tab separated file specified as argument to the `ReportName` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.pycopi.TimeCourse(K.kholodenko_model,\n",
    "                        ReportName=K.timecourse_report,\n",
    "                        ## Intervals*StepSize must equal End\n",
    "                        End=1000,    \n",
    "                        Intervals=50,\n",
    "                        StepSize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Plot and save results in the same directory as the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.pycopi.TimeCourse(K.kholodenko_model,\n",
    "                            ReportName=K.timecourse_report,\n",
    "                            End=1000,\n",
    "                            Intervals=50,\n",
    "                            StepSize=20,\n",
    "                            Plot='true',\n",
    "                            SaveFig='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    3. Collect results only for selected model variables only by passing a list to the `Metabolites` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.pycopi.TimeCourse(K.kholodenko_model,\n",
    "                            ReportName=K.timecourse_report,\n",
    "                            End=1000,\n",
    "                            Intervals=50,\n",
    "                            StepSize=20,\n",
    "                            Plot='true',\n",
    "                            SaveFig='true',\n",
    "                            Metabolites=['Mek1-P','Mek1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Play around with graph graphics. Note that these arguments are simply passed on to matplotlib and therefore more documenation can be found [here](http://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.pycopi.TimeCourse(K.kholodenko_model,\n",
    "                            ReportName=K.timecourse_report,\n",
    "                            End=1000,\n",
    "                            Intervals=50,\n",
    "                            StepSize=20,\n",
    "                            Plot='true',\n",
    "                            SaveFig='true',\n",
    "                            Metabolites=['Mek1-P','Mek1'],\n",
    "                            LineWidth=8,\n",
    "                            MarkerSize=15,\n",
    "                            MarkerColor='g',\n",
    "                            AxisSize=20,\n",
    "                            FontSize=25,\n",
    "                            LineStyle='-',\n",
    "                            XTickRotation=45,\n",
    "                            \n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is now going to be used in the next section for parameter estimation. To make things more interesting the following function can be used to add some noise the the data. Note that this method of adding noise is bias toward larger valued profiles but in the present context that doesn't really matter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas,numpy, FilePaths, os\n",
    "\n",
    "def add_noise(f, noise_factor=0.05):\n",
    "    '''\n",
    "    Add noise to time course data\n",
    "    \n",
    "    f:\n",
    "        Single experiment file to add noise too\n",
    "    \n",
    "    noise_factor:\n",
    "        limits the amount of noise to add. Must be float between 0 and 1\n",
    "        . default is 0.05 for 5% \n",
    "        \n",
    "    ==========\n",
    "    Returns: pandas.DataFrame containing noisy data \n",
    "    \n",
    "    '''\n",
    "    ## check file is real file\n",
    "    assert os.path.isfile(f),'{} is not a file'.format(f)\n",
    "    ## read into pandas\n",
    "    df=pandas.read_csv(f,sep='\\t')\n",
    "    ## Count number of data points to add noise too\n",
    "    ## remember to account for the time varible by minus 1 from column dimension \n",
    "    number_of_data_points= df.shape[0]*(df.shape[1]-1)\n",
    "    ## sample from uniform distribution 'number_of_data_points' times and assign to u vector\n",
    "    u= numpy.random.uniform(1-noise_factor,1+noise_factor,number_of_data_points)\n",
    "    ## reshape u vector into a 'u matrix'\n",
    "    u_matrix= u.reshape(df.shape[0],df.shape[1]-1)\n",
    "    ## remove the time colum but save as 't' variable for later\n",
    "    try:\n",
    "        \n",
    "        t=df['Time']\n",
    "        df.drop('Time',axis=1,inplace=True)\n",
    "        df_noise=pandas.DataFrame(u_matrix,columns=df.columns)\n",
    "    except KeyError:\n",
    "        return None\n",
    "    ## Check we have the corect shape for matrix\n",
    "    assert df.shape==df_noise.shape\n",
    "    ## Perform dot multiplication on two matrices to get noisy matrix\n",
    "    noise= df_noise.rmul(df,axis=0)\n",
    "    ## change index to be time\n",
    "    noise.index=t\n",
    "    ## return noisy vector\n",
    "    return noise\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ## instantiate FilePaths.KhodenkoExmaple to get necessary file paths\n",
    "    K=FilePaths.KholodenkoExample()\n",
    "    ## add noise to simulated kholodenko output and output to tab separated file. \n",
    "    add_noise(K.timecourse_report).to_csv(K.noisy_timecourse_report,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ParameterEstimation class is the only class in PyCoTools that is not fully automated as it required additional input from the user, such as which variables to estimate and between what boundaries. To use `ParameterEstimation`:\n",
    "        1. Instantiate instance of `ParameterEstimation`. This is where arguments and optional keyword arguments \n",
    "            such as `Method` and the method parameters are specified. \n",
    "        2. Use the `write_item_template()` method to output a file called 'fitItemTemplate.xlsx' \n",
    "            containing model variables, start values, boundaries and some additional information\n",
    "            which is used under the hood for creating fit items. \n",
    "        3. Use the `set_up()` method to set up the parameter estimation.This can be varified by \n",
    "            opening the model and looking at the parameter estimation \n",
    "        4. Optionally use the `run()` method to run the model with CopasiSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Instance of ParameterEstimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ParameterEstimation` class takes two positional arguments - absolute paths to 1) the model and 2) the parameter estimation data. The `ParameterEstimation` class can take lists of experiment files to set up multifit parameter estimations at the same time.\n",
    "\n",
    "For example to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Set up a parameter estimation with a single time course experiment and all other parameters default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PE= PyCoTools.pycopi.ParameterEstimation(K.kholodenko_model,\n",
    "                                         K.noisy_timecourse_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `PE.kwargs` can be used to view default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sorted(PE.kwargs.keys()):\n",
    "    print i,PE.kwargs[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2. Set up a parameter estimation with two time course experiment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## quickly modify the K.noisy_time_course_report name for demonstration \n",
    "import os\n",
    "second_timecourse= K.noisy_timecourse_report[:-4]+'2.txt'\n",
    "\n",
    "## quickly run another time course for demontration \n",
    "PyCoTools.pycopi.TimeCourse(K.kholodenko_model,ReportName=second_timecourse,End=1000,Intervals=50,StepSize=20)\n",
    "\n",
    "\n",
    "PE= PyCoTools.pycopi.ParameterEstimation(K.kholodenko_model,\n",
    "                                           [K.noisy_timecourse_report,second_timecourse])\n",
    "\n",
    "for i in sorted(PE.kwargs.keys()):\n",
    "    print i,PE.kwargs[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following varibales automatically change to lists of `len(ExperimentFiles)`:\n",
    "* RowOrientation\n",
    "* ExperimentType\n",
    "* FirstRow\n",
    "* NormalizeWeightsPerExperiment\n",
    "* RowContainingNames\n",
    "* Separator\n",
    "* WeightMethod\n",
    "\n",
    "These options (like all others) are described in detail on the [COPASI manual](http://copasi.org/Support/User_Manual/Tasks/Parameter_Estimation/).\n",
    "\n",
    "If any experiments are steady state experiments or if indivudal time course experiments are to be treated differently then custom lists need to be passed to these arguments. A user should consult the technical documentation distributed with PyCoTools for more details. \n",
    "\n",
    "Other important `kwargs` for parameter estimation are `Method` which specified which optimization algorithm to use as well as all options for all algorithms. To see which options are needed for which algorithm, simply open the `parameter estimation task` in the COPASI user interface and select the appropiate algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `PE.write_item_template()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import FilePaths\n",
    "K=FilePaths.KholodenkoExample()\n",
    "PE= PyCoTools.pycopi.ParameterEstimation(K.kholodenko_model,\n",
    "                                           K.noisy_timecourse_report, \n",
    "                                           Method='ParticleSwarm',\n",
    "                                           PopulationSize=150,\n",
    "                                           SwarmSize=200,\n",
    "                                           ReportName=K.PEData_file)\n",
    "\n",
    "PE.write_item_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that an the `ItemTemplateFilename` kwarg for `ParameterEstimation` can be used to specify custom filename, but we just use the default here. The fit item template looks like this:\n",
    "\n",
    "![name](fitItemTemplate.jpg)\n",
    "\n",
    "Delete rows of this matrix to remove corresponding parameters from the estimation (i.e. to fix variables) and change the boundaries and start values as you please. The start values column usually is not as important as multiple parameter estimations get run with random starting values. Do not modify the remaining columns on the right as these are used internally by PyCoTools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the `PE.set_up()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No configuration actually happens until you use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PE.set_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Parameter Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter estimation can be run multiple ways:\n",
    "    1. By opening COPASI, defining the usual plots and running like normal. \n",
    "    2. Use the `PE.run()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PE.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Its also useful to visualize experimental versus simulated output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PE= PyCoTools.pycopi.ParameterEstimation(K.kholodenko_model,\n",
    "                                           K.noisy_timecourse_report, \n",
    "                                           Method='ParticleSwarm',\n",
    "                                           PopulationSize=150,\n",
    "                                           SwarmSize=200,\n",
    "                                           Plot='true',\n",
    "                                           SaveFig='true')\n",
    "PE.set_up()\n",
    "PE.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we don't usually get a very good fit with the 'one run' approach so its often desirable to run the parameter estimation \n",
    "    4. Using the Scan task with a repeat item. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PE= PyCoTools.pycopi.ParameterEstimation(K.kholodenko_model,\n",
    "                                           K.noisy_timecourse_report, \n",
    "                                           Method='GeneticAlgorithm', #change to genetic algorithm (why not?)\n",
    "                                           PopulationSize=80,\n",
    "                                           NumberOfGenerations = 200,\n",
    "                                           Plot='false', #don't want to plot this time\n",
    "                                           SaveFig='false')\n",
    "PE.set_up()\n",
    "\n",
    "PyCoTools.pycopi.Scan(K.kholodenko_model,\n",
    "                      ScanType='repeat', #set up repeat item under scan. \n",
    "                      NumberOfSteps=3, #Run the parameter estimation task 10 times\n",
    "                      SubTask='parameter_estimation', #this is the default, but included here for demonstration anyway\n",
    "                      ReportType='parameter_estimation', ## report automatically set up within copasi. \n",
    "                      ReportName=K.PEData_file,\n",
    "                      Run='true') #run the scan task automatically in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most of the keyword arguments required to set up a task correspond to those which are required to set up the same task in the COPASI grahical interface. \n",
    "    5. If you do not have access to a cluster, it is sometimes desirable to copy the COPASI file and run multiple versions of the same file. \n",
    "    \n",
    "Lets define two simple functions, one to copy the `copasi_file` `n` times and another to create `n` output file names for collecting results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copy\n",
    "def copy_copasi(copasi_file,n):\n",
    "    '''\n",
    "    run a copasi file n times on the same computer \n",
    "    '''\n",
    "    sub_copasi_files_dct={}\n",
    "    copasi_path,copasi_filename=os.path.split(copasi_file)\n",
    "    for i in range(n):\n",
    "        new_cps=os.path.join(copasi_path,copasi_filename[:-4]+'{}.cps'.format(str(i)))\n",
    "        copy(copasi_file,new_cps)\n",
    "        sub_copasi_files_dct[i]= new_cps\n",
    "    return sub_copasi_files_dct\n",
    "\n",
    "def enumerate_PE_output(output_filename,n):\n",
    "    '''\n",
    "    make some more filenames for our analysis\n",
    "    '''\n",
    "    dct={}\n",
    "    dire,fle=os.path.split(output_filename)\n",
    "    output_dir=os.path.join(dire,'Results')\n",
    "    if os.path.isdir(output_dir)!=True:\n",
    "        os.mkdir(output_dir)\n",
    "    \n",
    "    for i in range(n):\n",
    "        new_file=os.path.join(output_dir,fle[:-4]+'{}.txt'.format(str(i)))\n",
    "        dct[i]=new_file\n",
    "    return dct\n",
    "\n",
    "n=3\n",
    "copasi_files=copy_copasi(K.kholodenko_model,n)\n",
    "result_files=enumerate_PE_output(K.PEData_file,n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter estimation task does not differ from when running only one `copasi_file` at a time and can stay the same. We now need to run the `Scan` task in a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    '''\n",
    "    Set up n repeat items with NumberOfSteps repeats of parameter estimation\n",
    "    '''\n",
    "    PyCoTools.pycopi.Scan(copasi_files[i],\n",
    "                      ScanType='repeat', \n",
    "                      NumberOfSteps=3, #run each model this number of times\n",
    "                      SubTask='parameter_estimation',\n",
    "                      ReportType='parameter_estimation', \n",
    "                      ReportName=result_files[i], #new output file name\n",
    "                      Run='false') #run the scan task automatically in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could set `Run='true'` but this would run each model in series. Instead it is possible to set `Run='false'` and pass a list of COPASI files to the `Run` class, using `multiprocess` mode. This opens a subprocess per copasi file and therefore can run multiple files at once. We also need to tell `Run` which task to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyCoTools\n",
    "for i in copasi_files.values():\n",
    "    PyCoTools.pycopi.Run(i,Mode='multiprocess',Task='scan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This approach is however quite primitive and care should be taken not to run too many copasi files at once. My own machine (8 cores, i7 processor and 16GB RAM) starts to get sluggish at around 6-8 models and becomes unusable with more than that until some files are finished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Lastly, for those with a SGE based cluster, `Run` can be set to `SGE` to run on the cluster. \n",
    "    \n",
    "Additional `Modes` can be easily written for clusters for other job schedulers by adding a method under the Run class and providing an extra option to the Run class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While COPASI is excellant for generation of parameter estimation data, users are largely left to their own devices when it comes to analysing this data. PyCoTools provides the `PEAnalysis` module which is designed specifically for quickly visualizing parameter estimation data, whether generated by COPASI or else where. \n",
    "\n",
    "\n",
    "The PEAnalysis module includes feature to:\n",
    "* Parse parameter estimation data into a python environment (`pandas.DataFrame`)\n",
    "* Quickly produce customizable:\n",
    "    * Boxplots\n",
    "    * Optimization performance graphs\n",
    "    * Histograms\n",
    "    * Scatter graphs\n",
    "\n",
    "The `InsertParameters` and `ParameterEstimation` classes are also useful in this context to visualize best fits against experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the time a user does not need to interface with the `PEAnalysis.ParsePEData` class since the other classes use it implicitly however it is useful sometimes if custom analyses are required, particurly if the data are all in separate files and need collecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyCoTools\n",
    "import FilePaths\n",
    "K=FilePaths.KholodenkoExample()\n",
    "data=PyCoTools.PEAnalysis.ParsePEData(K.PEData_file).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the ParsePEData class is used, it automatically prodces a `pickle` file containing a `pandas.DataFrame`. Its often good to set the `UsePickle='true'` and `OverwritePickle='false'` to speed up parsing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=PyCoTools.PEAnalysis.ParsePEData(K.PEData_file,UsePickle='true',OverwritePickle='false').data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a demonstration of this module works best with a large number of parameter estimation iterations, all the kinetic parameters in the `kholodenko2000` model were re-estimated 4000 times using COPASIs genetic algorithm with a population size of 300 and generation number of 1000. Only the kinetic variables were estimated using wide boundaries between 1e-6 to 1e6 and all the noisy data simualted above were used as experimental data. Additionally, the parameter estiamtions were run on the Newcastle university cluster using the scripts under the `Scripts` folder in the PyCoTools distribution. \n",
    "\n",
    "This data is available as a python pickle file [KholodenkoExamplePEData.pickle](https://github.com/CiaranWelsh/PyCoTools/tree/master/PyCoTools/Examples/KholodenkoExample). After download, put it in your working directory and make sure there is a pointer to it in the `FilePath.KholodenkoExample` class and use it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import FilePaths\n",
    "import PyCoTools\n",
    "K=FilePaths.KholodenkoExample()\n",
    "\n",
    "data=PyCoTools.PEAnalysis.ParsePEData(K.PEData_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualize Optimization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.EvaluateOptimizationPerformance(K.PEData_pickle,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a plot of the ordered likelihood (or RSS value) against iteration. The smooth curve indicates that the parameter estimation settings chosen for this problem are not a good choice. The absence of a monotonically increasing'step-like' shape suggests many optimizations are falling short of the minima that they are trying to find (Raue 2013). \n",
    "\n",
    "Solutions include:\n",
    "* Run more parameter estimations with different genetic algorithm specific settings (i.e. increase population size and number of generations for genetic algorithm)\n",
    "* Choose a  different algorithm and start again\n",
    "* Run a local optimization starting from the top $x$ best parameter sets.\n",
    "\n",
    "The latter is my favorite since it has the advantage that information is not lost from the *primary* parameter estimations.The logic it that even if the global optimizers haven't foud the 'house in the neibourhood' of the minima (whether local or global), they probably have found the neighbourhood. Therefore 'chasing' global primary parameter estimations with secondary local estimations is a good way to push the estimates a little further. \n",
    "\n",
    "Before proceding with runing 'chaser' secondary parameter estimations, lets visualize the data we already have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations Versus Experiment Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chaining together the `InsertParameter` class with the `ParameterEstimation` class using the `CurrentSolutionStatistics` method, setting `Plot='true'` and `RandomizeStartValues='false'`, we can visualize a plot of simulated versus experimental data.  Note we could also use `Index=1` to get a visual on the second best parameter set (and so on) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PEData=PyCoTools.PEAnalysis.ParsePEData(K.PEData_pickle)\n",
    "\n",
    "print 'best estimated parameters:\\n',PEData.data.iloc[0].sort_index()\n",
    "PyCoTools.pycopi.InsertParameters(K.kholodenko_model,ParameterPath=K.PEData_pickle,Index=0)\n",
    "PE=PyCoTools.pycopi.ParameterEstimation(K.kholodenko_model,K.noisy_timecourse_report,\n",
    "                                        Method='CurrentSolutionStatistics',\n",
    "                                        Plot='true',\n",
    "                                        SaveFig='false',\n",
    "                                        RandomizeStartValues='false') #important to turn this off\n",
    "PE.set_up() ## setup\n",
    "PE.run()    ## and run the current solution statistics parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs give further indication that our estimations can do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotBoxplot(K.PEData_pickle,SaveFig='false',NumPerPlot=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots are very good at highlighting the fact that some parameters are more 'identifiable' than others. We'll get to this issue later. \n",
    "\n",
    "Since a large portion of the parameter estimations are 'bad' runs its often useful to get a visual of the data with these runs omited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotBoxplot(K.PEData_pickle,SaveFig='false',NumPerPlot=15,TruncateMode='below_x',X=2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots now look tidier and the `RSS` much tighter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotHistogram(K.PEData_pickle_local,\n",
    "                                   Log10='true', ##plot on log10 scale\n",
    "                                   SaveFig='false',Bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs can also be truncated by top `X` percent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotHistogram(K.PEData_pickle_local,\n",
    "                                   Log10='true', \n",
    "                                   SaveFig='false',Bins=30,\n",
    "                                   TruncateMode='percent',X=10) ## Plot top 10% best runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, scatter graphs are useful for identifying relationships between variables. Usually the most interesting are linear relationships which can (but not *always*) indicate structural non-identifiability and plots of the objective function versus estimated parameters. These tend to least to 'identifiability signatures' which can indicate the identifiability status of a parameter ahead of an identifiability analysis. \n",
    "\n",
    "The `PlotScatters` class automatically plots all ${{N}\\choose{2}}$ pairs of estimated parameters and can therefore take some time with larger models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotScatters(K.PEData_pickle,SaveFig='false',\n",
    "                                  Log10='true') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotScatters(K.PEData_pickle,SaveFig='false',\n",
    "                                  Log10='false',TruncateMode='below_x',X=2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary 'Chaser' Parameter Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, secondary 'chaser' parameter estimations are good for pushing a global algorithm's results a little further and can sometimes be extremely informative. This process could be done manually for a select few parameter sets but realistically this isn't an option without programming tools. The general idea is the same as what would be done manually. First, insert parameters into a model, second setup and run a parameter estimation using a local method. Here is a function which does just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PyCoTools\n",
    "import FilePaths\n",
    "\n",
    "def runHJ(copasi_file,parameters,report_name,mode):\n",
    "    '''\n",
    "    copasi_file:\n",
    "        file to run hook and jeeves on \n",
    "    parameters:\n",
    "        dictionary of parameters. Keys must match model names, values are input parameters\n",
    "    report_name:\n",
    "        name of the report to output PE data.\n",
    "    mode:\n",
    "        either 'SGE' for SGE cluster or 'true' to run on current machine\n",
    "    '''\n",
    "    if 'RSS' in parameters.keys():\n",
    "        del parameters['RSS']\n",
    "    for i in parameters:\n",
    "        print i,':\\t',parameters[i]\n",
    "    PyCoTools.pycopi.InsertParameters(copasi_file,ParameterDict=parameters)\n",
    "    \n",
    "    PE=PyCoTools.pycopi.ParameterEstimation(copasi_file,K.noisy_timecourse_report,\n",
    "                                         Method='HookeJeeves',\n",
    "                                         IterationLimit=20000,\n",
    "                                         Tolerance=1e-30,\n",
    "                                         RandomizeStartValues='false',\n",
    "                                         Plot='true',\n",
    "                                         SaveFig='true',\n",
    "                                         UseTemplateStartValues='false',\n",
    "                                         )\n",
    "\n",
    "        \n",
    "    PE.set_up()\n",
    "    ## Run via scan task because this gives only best values in function\n",
    "    ## evaluations, rather than the periodic function evaluations as well\n",
    "    print '\\n\\n'\n",
    "    print report_name\n",
    "    print '\\n'\n",
    "    PyCoTools.pycopi.Scan(copasi_file,ScanType='repeat',Run=mode,\n",
    "                          NumberOfSteps=1,\n",
    "                          ReportName=report,\n",
    "                          ReportType='parameter_estimation')\n",
    "##USe the function\n",
    "if __name__=='__main__':\n",
    "    K=FilePaths.KholodenkoExample()\n",
    "    data=PyCoTools.PEAnalysis.ParsePEData(K.PEData_pickle).data\n",
    "    for i in range(int(data.shape[0]*0.025)):\n",
    "        print 'running index {} with starting RSS of {}'.format(i,data.iloc[i]['RSS'])\n",
    "        if os.path.isdir(K.local_PEData_dir)!=True:\n",
    "            os.mkdir(K.local_PEData_dir)\n",
    "        report=os.path.join(K.local_PEData_dir,'{}.txt'.format(i))\n",
    "        print runHJ(K.kholodenko_model,data.iloc[i].to_dict(),report,'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis of Secondary Parameter Estimation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the primary parameter estimations, the pickle file containing data from the secondary parameter estimations can be [downloaded](https://github.com/CiaranWelsh/PyCoTools/tree/master/PyCoTools/Examples/KholodenkoExample). To use this data, make sure there is a file path pointing to the newly downloaded file in your own version of `FilePaths.KholodenkoExample()` so that you can access the pickle file easily from any script. Alternatiely, just assign a string variable containing the path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PyCoTools\n",
    "import FilePaths\n",
    "%matplotlib inline\n",
    "K=FilePaths.KholodenkoExample()\n",
    "print K.PEData_pickle_local\n",
    "PyCoTools.PEAnalysis.EvaluateOptimizationPerformance(K.PEData_pickle_local,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotBoxplot(K.PEData_pickle_local,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotHistogram(K.PEData_pickle_local,Bins=50,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotScatters(K.PEData_pickle_local,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Second Iteration of Parameter Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that the 'experimental data' used in these parameter estimations were simulated from the model being fit, the fits, overall this round of fitting was quite poot. This is actually fairly for a first round of parameter estimations - probably underscoring the fact that parameter estimation is a difficult problem in systems biology!\n",
    "\n",
    "A good next step after running and visualizing the parameter estimation data is to run calculate profile likelihoods around the best (or best few) parameter sets. However, with these parameter sets there is little point since our best parameter set isn't really a minimum, local or otherwise - profile likelihoods around this set would just look like noise. \n",
    "\n",
    "A good alternative in this situation is to narrow the search space using the information we already have in the hope that we'll be more confident after the next iteration that profile likelihoods will actually provide useful information. Its not always straight forward how to do this and sometimes a bad decision, such as an inappropiately constrained parameter, can restrict your solution to a sub-optimal parameter set. Experience with this leads me to act cautiously when fixing parameter estimates or modifying a parameters estimation boundaries. \n",
    "\n",
    "With this in mind, I've chosen to fix three parameters, `KK3`,`V2` and`V4`, mostly based on their tight boxplots but also because they have 'reasonable' values, unlike `KK6` which has a reasonably tight distribution but unusually high mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyCoTools\n",
    "import FilePaths\n",
    "K=FilePaths.KholodenkoExample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Optimization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.EvaluateOptimizationPerformance(K.PEDataGlobal2,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotBoxplot(K.PEDataGlobal2,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotHistogram(K.PEDataGlobal2,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Scatter Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotScatters(K.PEDataGlobal2,Log10='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Matplotlib alo support a number of [colour maps](http://matplotlib.org/1.2.1/examples/pylab_examples/show_colormaps.html) which an be used for alternative colouring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tomorrow I implement colour  maps on all of my graphs. Also produce a more targeted scatter class, so we don't have to run all at once? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Second Iteration: Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.EvaluateOptimizationPerformance(K.PEDataLocal2,Log10='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotBoxplot(K.PEDataLocal2,Log10='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PyCoTools.PEAnalysis.PlotHistogram(K.PEDataLocal2,Log10='true',Bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyCoTools\n",
    "PyCoTools.PEAnalysis.PlotScatters(K.PEDataLocal2,Log10='true')#use Show='true' to show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1302c91bb523>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1302c91bb523>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    PyCoTools.PEAnalysis.(K.PEDataLocal2,Log10='true')\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import PyCoTools\n",
    "PyCoTools.PEAnalysis.(K.PEDataLocal2,Log10='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Refernces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Raue, A., Schilling, M., Bachmann, J., Matteson, A., Schelke, M., Kaschek, D., Hug, S., Kreutz, C., Harms, B.D., Theis, F.J., Klingmüller, U. and Timmer, J. (2013) 'Lessons Learned from Quantitative Dynamical Modeling in Systems Biology', PLoS ONE, 8(9), p. e74335.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
