{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PEData=PyCoTools.PEAnalysis.ParsePEData(K.PEData_pickle)\n",
    "\n",
    "print 'best estimated parameters:\\n',PEData.data.iloc[0].sort_index()\n",
    "PyCoTools.pycopi.InsertParameters(K.kholodenko_model,ParameterPath=K.PEData_pickle,Index=0)\n",
    "PE=PyCoTools.pycopi.ParameterEstimation(K.kholodenko_model,K.noisy_timecourse_report,\n",
    "                                        Method='CurrentSolutionStatistics',\n",
    "                                        Plot='true',\n",
    "                                        SaveFig='false',\n",
    "                                        RandomizeStartValues='false') #important to turn this off\n",
    "PE.set_up() ## setup\n",
    "PE.run()    ## and run the current solution statistics parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions include:\n",
    "* Run more parameter estimations with different genetic algorithm specific settings (i.e. increase population size and number of generations for genetic algorithm)\n",
    "* Choose a  different algorithm and start again\n",
    "* Run a local optimization starting from the top $x$ best parameter sets.\n",
    "\n",
    "The latter is my favorite since it has the advantage that information is not lost from the *primary* parameter estimations.The logic it that even if the global optimizers haven't foud the 'house in the neibourhood' of the minima (whether local or global), they probably have found the neighbourhood. Therefore 'chasing' global primary parameter estimations with secondary local estimations is a good way to push the estimates a little further. \n",
    "\n",
    "Before proceding with runing 'chaser' secondary parameter estimations, lets visualize the data we already have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Scatter graphs are useful for identifying relationships between variables. Usually the most interesting are linear relationships which can (but not *always*) indicate structural non-identifiability and plots of the objective function versus estimated parameters. These tend to least to 'identifiability signatures' which can indicate the identifiability status of a parameter ahead of an identifiability analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
